---
title: 'Homework: Numerical linear algebra'
output:
  html_document:
    df_print: paged
  html_notebook:
    code_folding: none
    highlight: textmate
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      bA: "{\\boldsymbol{A}}",
      bx: "{\\boldsymbol{x}}",
      bb: "{\\boldsymbol{b}}"
    }
  }
});
</script>


```{r setup, include=FALSE}
source(file.path("..", "R", "util.R"))

required_packages <- c("microbenchmark")
install_and_load_packages(required_packages)
```


# Exercise 1: Comparing different numerical linear algebra algorithms for solving linear systems

In this exercise, we consider the problem of solving a linear system $\bA \bx = \bb$ for $\bx$.
We compare the three methods we learned in the class: LU, Cholesky, and QR decompositions.
(Of course, LU applies to more general systems and QR can be used to solve least squares, but here we focus on positive definite systems.)

## Part A: Racing for solution &mdash; speed comparison 

We first compare their computational speed. 
Fill in the code below and `bench::mark()` the three algorithms.

**Questions:**
What are relative speeds among the algorithms?
Do relative speeds agree with what you expect from the complexity analyses?
If not (quite), why might that be?

**My answer: **
Theoretically, the overall costs of solving a single linear system involving symmetric positive definite matrix via LU decomposition, Cholesky decomposition, and QR decomposition are $O(\frac{2}{3}p^3)$, $O(\frac{1}{3}p^3)$, and $O(\frac{4}{3}p^3)$. However, the relative speeds do not agree with the theoretical costs, where LU decomposition using solve() takes 134ms (median), Cholesky using chol() and backsolve() takes 173ms (median), and QR using qr.solve() takes 496ms (median). solve() is the fastest because in this case we provided a symmetric positive definite matrix in the system and solve() uses an optimized algorithm to directly use Cholesky decomposition and backward and forward substitution for such matrices, which is faster than using the built-in functions chol() and backsolve() and allocating space to store the upper triangular matrix. 

**Note:**
I misspoke about the R's built-in `chol()` function during the lecture:
when applied to a positive-definite `A`, the function actually returns an _upper-triangular_ matrix `R` such that `t(R) %*% R == A`.

```{r, eval=FALSE}
# Import the `rand_positive_def_matrix` function
source(file.path("R", "random_matrix.R"))

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
b <- rnorm(mat_size)

solve_via_cholesky <- function(A, b) {
    upper <- chol(A)
    backsolve(upper, backsolve(upper, b, transpose = TRUE))
}

bench::mark(
  solve(A, b)
)
bench::mark(
  solve_via_cholesky(A, b)
)
bench::mark(
  qr.solve(A, b)
)
```

## Part B: Competition in terms of numerical accuracy/stability

We now compare the three methods in terms of numerical accuracy/stability.
To this end, we set up the following simulation study. 
We first generate a "ground truth" solution vector $\bx_0$.
We then compute an "numerical-error-free" $\bb = \bA \bx_0$ by carrying out the matrix-vector multiplication using the `long double` type, which (on most hardware and compilers) provides [additional 12 bits of precision](https://en.wikipedia.org/wiki/Extended_precision#x86_extended_precision_format).
Of course, the vector $\bb$ computed as such still suffers from numerical errors, but the idea is that the numerical errors from this high-precision matrix-vector multiplication is much smaller than the errors caused by numerically solving $\bA \bx = \bb$ for $\bx$.
We can thus assess the accuracy of the three solvers by comparing the numerically computed $\bx$ to the ground truth $\bx_0$.

### Task &#x2F00;

First compare the outputs of matrix-vector multiplication $\bx \to \bA \bx$ using `double` and `long double` using the provided Rcpp functions.

**Questions:**
What is the relative difference in $\ell^2$-norm? 
How about the coordinate-wise relative differences?
Are the observed magnitudes of the differences what you'd expect?

**My answer:**
The relative difference in l2-norm is 1.19e-15 ($\approx 2^{-50}$). The maximum of coordinate-wise relative differences is 4.65e-13 ($\approx 2^{-41}$). The observed magnitudes of differences match with what I expect because multiplication using `long double` provides additional 12 bits of precision after 53rd binary digits. So it is reasonable that the relative difference in l2-norm and coordinate-wise relative differences are around the scale of $2^{-52}$. 

```{r, eval=FALSE}
Rcpp::sourceCpp(file.path("src", "matvec_double.cpp"))
Rcpp::sourceCpp(file.path("src", "matvec_ldouble.cpp"))

set.seed(1918)

mat_size <- 1024L
A <- rand_positive_def_matrix(mat_size, cond_num = 1e3)
x <- rnorm(mat_size)

l2_norm <- function(x) {
  sqrt(sum(x^2))
}

calc_diffs <- function(x1, x2) {
  x1_norm <- l2_norm(x1) 
  l2_norm_diff <- l2_norm(x1 - x2)
  l2_norm_rel_diff <- l2_norm_diff / x1_norm
  coord_rel_diff <- abs(x1 - x2) / abs(x1)
  return(list(l2_norm_rel_diff = l2_norm_rel_diff, coord_rel_diff = coord_rel_diff))
}

Ax_double <- matvec_double(A, x)
Ax_ldouble <- matvec_ldouble(A, x)

diffs <- calc_diffs(Ax_ldouble, Ax_double)
print(paste0("l2-norm relative difference: ", diffs$l2_norm_rel_diff))
print(paste0("Max coordinate-wise relative differences: ", max(diffs$coord_rel_diff)))

```

### Task &#x2F06;

Now randomly generate $\bA$ so that its condition number is $10^6$.
Then solve a positive-definite system $\bA \bx = \bb$ for $\bx$ using the three algorithms and compare their outputs to the ground truth $\bx_0$.

**Questions:**
Which algorithm appears to be more accurate than the others? 
Visually demonstrate your answer.

**My answer:**
It appears that LU decomposition using solve() is more accurate than the others, but all three methods yield comparably small relative errors. Please refer to the saved png file in the folder.

```{r, eval=FALSE}
set.seed(1918)
cond_num <- 1e6

# Larger matrices could incur substantial computational time under base R BLAS
mat_size <- 1024L 

A <- rand_positive_def_matrix(mat_size, cond_num)
x <- rnorm(mat_size)

b <- matvec_ldouble(A, x)
x_lu <- solve(A, b)
x_chol <- solve_via_cholesky(A, b)
x_qr <- qr.solve(A, b)

diffs_lu <- calc_diffs(x, x_lu)
diffs_chol <- calc_diffs(x, x_chol)
diffs_qr <- calc_diffs(x, x_qr)

df <- data.frame(
  Method = c(rep("LU", length(diffs_lu$coord_rel_diff)),
             rep("Cholesky", length(diffs_chol$coord_rel_diff)),
             rep("QR", length(diffs_qr$coord_rel_diff))),
  CoordinateDifference = log10(c(diffs_lu$coord_rel_diff, diffs_chol$coord_rel_diff, diffs_qr$coord_rel_diff))
)

library(ggplot2)
png("num_linalg_task2.png")
p2 <- ggplot(df, aes(x = Method, y = CoordinateDifference)) +
  geom_boxplot() +
  ylab("log10(Coordinate-wise relative differences)") +
  ggtitle("Comparison of accuracy between methods")
ggsave("num_linalg_task2.png", plot = p2)
```

### Task &#x4E09;

In Task &#x2F06;, we compared the three algorithms in one randomly generated example.
Now we consider a more systematic (though hardly comprehensive) comparison via repeated simulations.
We also vary the condition number of $\bA$ and assess whether the results would hold across varying degrees of ill-conditioning.

**Questions/To-do's:**

* Using the starter code provided, calculate various summary measures of the numerical errors.
* Integrate into the provided code one another (or more, if you like) meaningful metric(s) of your choice to summarize the numerical error.
* Visually explore how the three algorithms compare with each other in their accuracy. See if different error metrics tell different stories; they might or might not.
* Vary the condition number in the range $10^6 \sim 10^{12}$, e.g. by trying $10^6$, $10^9$, and $10^{12}$.
* Do you see any patterns in the numerical errors across the three algorithms, metrics, and/or condition numbers? Show some plots to support your conclusion.

**My answer:**
When the condition number increases, the coordinate-wise relative error increases across three algorithms and metrics. That's because the problem is more ill-conditioned for large k and the algorithms are more likely to suffer from numiercal instability. When the condition number is low or the problem is well-conditioned, LU decomposition using solve() yields smallest relative error, followed by Cholesky and QR decomposition. When the condition number is high, Cholesky decomposition has the highest numerical statbility for a system with positive definite matrices, and QR decomposition has the highest relative error. The patterns remain similar across the five metrics. Please refer to the saved png files in the folder.

**Note:** 
The QR solver will throw an error when the system is ill-condition enough that the numerical solution might not be very accurate. 
To force it to return the solution in any case, set `tol = .Machine$double.eps`.

```{r, eval=FALSE}
# Utility functions for bookkeeping simulation results.
source(file.path("R", "num_linalg_sim_study_helper.R"))

n_sim <- 32L
mat_size <- 512L
cond_nums <- c(1e6, 1e9, 1e12)
metrics <- c("norm", "median", "five_percentile", "ninety_five_percentile", "mean")
method_list <- c("lu", "chol", "qr")
rel_cond <- list()

for (cond_num in cond_nums) {
  rel_error_list <- lapply(
    c("lu", "chol", "qr"), 
    function(method) pre_allocate_error_list(n_sim, metrics)
  )
  names(rel_error_list) <- method_list

  for (sim_index in 1:n_sim) {
    A <- rand_positive_def_matrix(mat_size, cond_num)
    x <- rnorm(mat_size) 
    b <- matvec_ldouble(A, x)
    x_approx <- list( 
        lu = solve(A, b),
        chol = solve_via_cholesky(A, b),
        qr = qr.solve(A, b, tol = .Machine$double.eps)
    )
    for (method in c("lu", "chol", "qr")) {
      rel_error <- lapply(
        metrics, 
        function (metric) calc_rel_error(x, x_approx[[method]], metric)
      )
      names(rel_error) <- metrics
      for (metric in names(rel_error)) {
        rel_error_list[[method]][[metric]][sim_index] <- rel_error[[metric]]
      }
    }
  }
  rel_cond[[as.character(cond_num)]] <- rel_error_list
}

```


```{r}
#visually compare errors
library(gridExtra)
cond_nums <- c(1e6, 1e9, 1e12)
for (cond_num in as.character(cond_nums)) {
  rel_error_list <- rel_cond[[cond_num]]
  plots <- list()
  for (metric in metrics) {
    df <- data.frame(
      Method = c(
        rep("LU", n_sim),
        rep("Cholesky", n_sim),
        rep("QR", n_sim)
      ),
      Difference = log10(c(rel_error_list[["lu"]][[metric]], rel_error_list[["chol"]][[metric]], rel_error_list[["qr"]][[metric]]))
    )

    p <- ggplot(df, aes(x = Method, y = Difference)) +
      geom_boxplot() +
      ylab(paste0("log10(", metric, " of Coordinate-wise relative differences)")) +
      ggtitle(paste0("Comparison of accuracy between methods, k = ", cond_num)) + 
      theme(plot.title = element_text(size = 7), 
            axis.title.x = element_text(size = 7),
            axis.title.y = element_text(size = 5))
    plots[[metric]] <- p
  }
  overall_plot <- do.call(grid.arrange, c(plots, ncol = 3))
  ggsave(paste0("num_linalg_task3_", cond_num, ".png"), overall_plot)
}

```

